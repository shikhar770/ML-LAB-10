{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Lab_9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET8dyRgZHqC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYOAUo1JIcWw",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "256a91d9-d0f4-4e83-e1ef-600c31103368"
      },
      "source": [
        "uploaded = files.upload()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a30fe250-62d8-4847-9a0b-3d99640248be\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-a30fe250-62d8-4847-9a0b-3d99640248be\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving mnist_train.csv to mnist_train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXFrvVqFJKwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(filepath):\n",
        "  df = pd.read_csv(filepath)\n",
        "  df.head()\n",
        "  Y = df['5'].to_numpy()\n",
        "  del df['5']\n",
        "  X=df.to_numpy()\n",
        "  return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1UY5CQONxQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = load_data(\"mnist_train.csv\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\\\n",
        "                X, y, test_size=0.3, random_state=42)\n",
        "#One hot encoding of training labels \n",
        "Labels=pd.get_dummies(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBFj4Dp3N2_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Perceptron():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "\n",
        "\n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            x=self.softmax(z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "    \n",
        "    \n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            prev_term=self.delta_mll(y,self.y_pred)  \n",
        "            # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "            self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))    \n",
        "\n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            x=self.softmax(z) \n",
        "        return np.argmax(x,axis=1)       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D519_DBLORll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "2ae0ea20-268a-4718-99fb-8def61b11637"
      },
      "source": [
        "n=Perceptron(X_train,Labels)\n",
        "n.connect(X_train,Labels)\n",
        "n.train(batches=1000,lr=0.2,epoch=30)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.002298718486991328\n",
            "1 0.0021641996496077633\n",
            "2 0.0016796934109130444\n",
            "3 0.001569051233691067\n",
            "4 0.0013876988023833635\n",
            "5 0.0013319218661356203\n",
            "6 0.0012887546525424491\n",
            "7 0.0013519341788540819\n",
            "8 0.0012459237464419461\n",
            "9 0.0010918197979889847\n",
            "10 0.0009981321950729828\n",
            "11 0.0009015582562181782\n",
            "12 0.0008396411824288056\n",
            "13 0.0007865695195698346\n",
            "14 0.0007058160210851129\n",
            "15 0.0006336635656405407\n",
            "16 0.0005354456331380514\n",
            "17 0.0004158638686677986\n",
            "18 0.0003078721986552477\n",
            "19 0.0002230204473203623\n",
            "20 0.0001642537606451533\n",
            "21 0.00012824567040242577\n",
            "22 0.00010756066197949124\n",
            "23 9.341291403148477e-05\n",
            "24 8.327525826675173e-05\n",
            "25 7.563717296388393e-05\n",
            "26 6.948324005243564e-05\n",
            "27 6.438023664146283e-05\n",
            "28 6.0283291085679186e-05\n",
            "29 5.722958885903194e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uJGW7aROtoM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f2153317-c99a-4511-8fc3-68d648a83d10"
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([306, 320, 296, 347, 329, 286, 290, 314, 232, 280]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T09ABjojOwcC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38ee5df3-2733-444d-f0f9-750b0c37cf68"
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 88.56666666666666 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ-4NZogO0K8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    \"\"\"\n",
        "    size: Number of nodes in the hidden layer \n",
        "    activation: name of activation function for the layer\n",
        "    \"\"\"\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class SingleLayerNeuralNetwork():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "    def activation(self,name,z,derivative=False):\n",
        "        \n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "        \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "\n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                \n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "\n",
        "        return np.argmax(x,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIOoiv0NPl-R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "outputId": "6e8e96ec-c3ad-4a70-bc1b-70f242d28037"
      },
      "source": [
        "n=SingleLayerNeuralNetwork(X_train,Labels)\n",
        "l1=Layer(100)\n",
        "n.connect(X_train,l1)\n",
        "n.connect(l1,Labels)\n",
        "n.train(batches=1000,lr=0.1,epoch=50)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0011412718333489722\n",
            "1 0.0008161776492881548\n",
            "2 0.0006006029690733042\n",
            "3 0.000458281856319554\n",
            "4 0.00032915334484810086\n",
            "5 0.0002546742820719631\n",
            "6 0.0002073814414453328\n",
            "7 0.00016658334813247016\n",
            "8 0.00013206434238700093\n",
            "9 0.00010356480541226177\n",
            "10 8.1615137195873e-05\n",
            "11 6.571425324445122e-05\n",
            "12 5.502261515356171e-05\n",
            "13 4.7813474195475635e-05\n",
            "14 4.242168839305582e-05\n",
            "15 3.807914105493015e-05\n",
            "16 3.451952661185515e-05\n",
            "17 3.154949516678256e-05\n",
            "18 2.898165600001289e-05\n",
            "19 2.6748354010187025e-05\n",
            "20 2.483138380716246e-05\n",
            "21 2.3165253136444853e-05\n",
            "22 2.1680826637312494e-05\n",
            "23 2.033398740005741e-05\n",
            "24 1.9102788577463875e-05\n",
            "25 1.7975775211642234e-05\n",
            "26 1.6944382877391137e-05\n",
            "27 1.6000667726859057e-05\n",
            "28 1.5137007898666522e-05\n",
            "29 1.4346072157297521e-05\n",
            "30 1.3620833823173254e-05\n",
            "31 1.2954651164209906e-05\n",
            "32 1.234137393139731e-05\n",
            "33 1.1775423918935933e-05\n",
            "34 1.1251826403119013e-05\n",
            "35 1.0766196259664783e-05\n",
            "36 1.0314694019860664e-05\n",
            "37 9.893967266702639e-06\n",
            "38 9.501088487828314e-06\n",
            "39 9.133495913685383e-06\n",
            "40 8.788940446721532e-06\n",
            "41 8.465439658713352e-06\n",
            "42 8.161238685283537e-06\n",
            "43 7.874777324666406e-06\n",
            "44 7.604662480884427e-06\n",
            "45 7.349645103779443e-06\n",
            "46 7.108600867505366e-06\n",
            "47 6.880513940830093e-06\n",
            "48 6.664463311309929e-06\n",
            "49 6.459611220967e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGb10d0ePrTi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "61aab6cd-6ed6-415c-fac5-c45fa1092199"
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([299, 334, 298, 336, 309, 269, 285, 333, 250, 287]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUhx4Rd6Q98J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c42b336-3c2a-4cb8-907a-3067e7833634"
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 91.9 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbOiILwtRJUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    \"\"\"\n",
        "    size: Number of nodes in the hidden layer \n",
        "    activation: name of activation function for the layer\n",
        "    \"\"\"\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class DoubleLayerNeuralNetwork():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "\n",
        "    def activation(self,name,z,derivative=False):\n",
        "        \n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        \n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "\n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "\n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "                \n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "\n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        return np.argmax(x,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PNbQC0VTJ1E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "c62f68d3-4817-4df6-b5e9-7cad147ce874"
      },
      "source": [
        "n=DoubleLayerNeuralNetwork(X_train,Labels)\n",
        "l1=Layer(100)\n",
        "l2=Layer(100)\n",
        "n.connect(X_train,l1)\n",
        "n.connect(l1,l2)\n",
        "n.connect(l2,Labels)\n",
        "n.train(batches=1000,lr=0.1,epoch=20)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0005928818116569626\n",
            "1 0.0005485662431111254\n",
            "2 0.00025586523713256497\n",
            "3 0.0001231487022012527\n",
            "4 8.968613074244082e-05\n",
            "5 6.105672900731822e-05\n",
            "6 3.6549971982974445e-05\n",
            "7 2.477612142878513e-05\n",
            "8 1.9183854173501487e-05\n",
            "9 1.594948227695937e-05\n",
            "10 1.4065795737219559e-05\n",
            "11 1.3452859969858714e-05\n",
            "12 1.3269545926224059e-05\n",
            "13 1.3060974651241815e-05\n",
            "14 1.2459270685705511e-05\n",
            "15 1.1226114188503182e-05\n",
            "16 9.697489706699124e-06\n",
            "17 8.328420691318528e-06\n",
            "18 7.164386300700114e-06\n",
            "19 6.195069486571837e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR_a_5kiTa6e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7bc51526-2a0e-4677-fe13-7872db4771ee"
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([294, 325, 317, 307, 321, 249, 295, 311, 298, 283]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdD2Zs7HT3LM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74a83b86-b327-4506-e466-014a4606bcfe"
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 90.83333333333333 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3cQtZvtT9tR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    \"\"\"\n",
        "    size: Number of nodes in the hidden layer \n",
        "    activation: name of activation function for the layer\n",
        "    \"\"\"\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class NeuralNetworkActivations():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "    \n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "\n",
        "    def activation(self,name,z,derivative=False):\n",
        "        \n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        elif name=='relu':\n",
        "            if derivative==False:\n",
        "                return np.maximum(0.0,z)\n",
        "            else:\n",
        "              z[z<=0] = 0.0\n",
        "              z[z>0] = 1.0\n",
        "              return z\n",
        "        elif name=='tanh':\n",
        "          if derivative==False:\n",
        "                return np.tanh(z)\n",
        "          else:\n",
        "                return 1.0 - (np.tanh(z)) ** 2\n",
        "\n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "\n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "\n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "\n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        return np.argmax(x,axis=1)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNHC1hz8VBFv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "5503d4cd-d46d-40b4-cecb-74a33b86afc8"
      },
      "source": [
        "n=NeuralNetworkActivations(X_train,Labels)\n",
        "l1=Layer(100,'sigmoid')\n",
        "l2=Layer(50, 'tanh')\n",
        "n.connect(X_train,l1)\n",
        "n.connect(l1,l2)\n",
        "n.connect(l2,Labels)\n",
        "n.train(batches=1000,lr=0.1,epoch=20)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0009618017038127462\n",
            "1 0.0005138348959874894\n",
            "2 0.0006278362935364526\n",
            "3 9.963197676935971e-05\n",
            "4 8.184694164502816e-05\n",
            "5 6.309971453119264e-05\n",
            "6 9.611697383032649e-06\n",
            "7 0.00017172124808669873\n",
            "8 8.073072465005046e-06\n",
            "9 9.100178871872507e-05\n",
            "10 3.573921593162221e-06\n",
            "11 0.00032729512909518253\n",
            "12 3.159565728452364e-05\n",
            "13 0.0003605950191712199\n",
            "14 3.256335601396975e-05\n",
            "15 0.0002310313394324728\n",
            "16 3.0488324257338034e-06\n",
            "17 1.9161662225967365e-06\n",
            "18 0.0002688453260792655\n",
            "19 3.9883467769663975e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3dnGC9yVgXO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3fbbfec3-0e80-4f96-8aec-775206fbab7d"
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([301, 332, 291, 322, 303, 245, 310, 333, 258, 305]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GjeVN08VqEJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "183a28c4-d5f6-4d27-c889-72276b37f304"
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 92.23333333333333 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfrojmGMVv8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer():\n",
        "    \"\"\"\n",
        "    size: Number of nodes in the hidden layer \n",
        "    activation: name of activation function for the layer\n",
        "    \"\"\"\n",
        "    def __init__(self,size,activation='sigmoid'): \n",
        "        self.shape=(1,size)\n",
        "        self.activation=activation\n",
        "                \n",
        "class NeuralNetworkMomentum():\n",
        "    def __init__(self,x,y):\n",
        "        \"\"\"\n",
        "        x is 2d array of input images\n",
        "        y are one hot encoded labels \n",
        "        \"\"\"\n",
        "        self.x=x/255   # Divide by 255 to normalise the pixel values (0-255)\n",
        "        self.y=y\n",
        "        self.weights=[]\n",
        "        self.bias=[]\n",
        "        self.outputs=[]\n",
        "        self.derivatives=[]\n",
        "        self.activations=[]\n",
        "        self.delta_weights=[]\n",
        "        self.delta_bias=[]\n",
        "\n",
        "    def connect(self,layer1,layer2):\n",
        "        \"\"\"layer 2 of shape 1xn\"\"\"\n",
        "        #Initialise weights,derivatives and activation lists\n",
        "        self.derivatives.append(np.random.uniform(0,0.1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.weights.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.bias.append(np.random.uniform(-1,1,size=(layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.delta_weights.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
        "        self.delta_bias.append(np.zeros((layer1.shape[1]+1,layer2.shape[1])))\n",
        "        if isinstance(layer2,Layer):\n",
        "            self.activations.append(layer2.activation)\n",
        "\n",
        "    def activation(self,name,z,derivative=False):\n",
        "        \n",
        "        #implementation of various activation functions and their derivatives\n",
        "        if name=='sigmoid':\n",
        "            if derivative==False:\n",
        "                return 1/(1+np.exp(-z))\n",
        "            else:\n",
        "                return z*(1-z)\n",
        "        elif name=='relu':\n",
        "            if derivative==False:\n",
        "                return np.maximum(0.0,z)\n",
        "            else:\n",
        "              z[z<=0] = 0.0\n",
        "              z[z>0] = 1.0\n",
        "              return z\n",
        "        elif name=='tanh':\n",
        "          if derivative==False:\n",
        "                return np.tanh(z)\n",
        "          else:\n",
        "                return 1.0 - (np.tanh(z)) ** 2\n",
        "\n",
        "    def softmax(self,z):\n",
        "        e=np.exp(z)\n",
        "        return e/np.sum(e,axis=1).reshape(-1,1) \n",
        "    \n",
        "    def max_log_likelihood(self,y_pred,y):\n",
        "        \"\"\"cross entropy\"\"\"\n",
        "        return y*np.log(y_pred)\n",
        "    \n",
        "    def delta_mll(self,y,y_pred):\n",
        "        \"\"\"derivative of cross entropy\"\"\"\n",
        "        #return y*(y_pred-1)\n",
        "        return y_pred-y\n",
        "    \n",
        "    def forward_pass(self,x,y,weights,bias):\n",
        "        cost=0\n",
        "        self.outputs=[]\n",
        "        for i in range(len(weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            self.outputs.append(x) #append without adding ones array\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),weights[i]+bias[i])\n",
        "            if i==len(weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        self.outputs.append(x)\n",
        "        self.y_pred=x\n",
        "        \n",
        "        temp=-self.max_log_likelihood(self.y_pred,y)\n",
        "        cost=np.mean(np.sum(temp,axis=1))\n",
        "        return cost\n",
        "\n",
        "    def backward_pass(self,y,lr):\n",
        "        for i in range(len(self.weights)-1,-1,-1):\n",
        "            ones_array=np.ones(len(n.outputs[i])).reshape(len(n.outputs[i]),1)\n",
        "            if i==len(self.weights)-1:\n",
        "                prev_term=self.delta_mll(y,self.y_pred)  \n",
        "                # derivatives follow specific order,last three terms added new,rest from previous term  \n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))   \n",
        "            else:\n",
        "                prev_term=np.dot(prev_term,self.weights[i+1][1:].T)*self.activation(self.activations[i],self.outputs[i+1],derivative=True)\n",
        "                self.derivatives[i]=np.dot(prev_term.T,np.append(ones_array,self.outputs[i],axis=1))\n",
        "            self.weights[i]=self.weights[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "            self.bias[i]=self.bias[i]-lr*((self.derivatives[i].T)/len(y))\n",
        "    \n",
        "    def train(self,batches,lr=1e-3,epoch=10):\n",
        "        \"\"\"number of batches to split data in,Learning rate and epochs\"\"\"\n",
        "        for epochs in range(epoch):\n",
        "            samples=len(self.x)\n",
        "            c=0\n",
        "            for i in range(batches):\n",
        "              x_batch=self.x[int((samples/batches)*i):int((samples/batches)*(i+1))]\n",
        "              y_batch=self.y.loc[int((samples/batches)*i):int((samples/batches)*(i+1))-1]\n",
        "              \n",
        "              c=self.forward_pass(x_batch,y_batch,self.weights,self.bias)\n",
        "              self.backward_pass(y_batch,lr)\n",
        "            print(epochs,c/batches)\n",
        "\n",
        "    def predict(self,x):\n",
        "        \"\"\"input: x_test values\"\"\"\n",
        "        x=x/255\n",
        "        for i in range(len(self.weights)):\n",
        "            samples=len(x)\n",
        "            ones_array=np.ones(samples).reshape(samples,1)\n",
        "            z=np.dot(np.append(ones_array,x,axis=1),self.weights[i]+self.bias[i])\n",
        "            if i==len(self.weights)-1:\n",
        "                x=self.softmax(z)\n",
        "            else:\n",
        "                x=self.activation(self.activations[i],z)\n",
        "        return np.argmax(x,axis=1)\n",
        "     \n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wvn-8zpSbfEu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "5e453736-ef33-4686-aa54-28ea4eac3c5d"
      },
      "source": [
        "n=NeuralNetworkMomentum(X_train,Labels)\n",
        "l1=Layer(100,'sigmoid')\n",
        "l2=Layer(50, 'tanh')\n",
        "n.connect(X_train,l1)\n",
        "n.connect(l1,l2)\n",
        "n.connect(l2,Labels)\n",
        "n.train(batches=500,lr=0.1,epoch=20)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.0015567132805731226\n",
            "1 0.0013669061349866575\n",
            "2 0.0011136194364365468\n",
            "3 0.0010659147774713749\n",
            "4 0.0010188832318904152\n",
            "5 0.0009797784921970803\n",
            "6 0.0008640774497339289\n",
            "7 0.000918254878467305\n",
            "8 0.00081438544316252\n",
            "9 0.0006414252138850572\n",
            "10 0.0004567359777407405\n",
            "11 0.0003081952652590399\n",
            "12 0.00023189979708071863\n",
            "13 0.00016756836128454554\n",
            "14 0.00012719381433711892\n",
            "15 9.945734740933994e-05\n",
            "16 7.65693987907423e-05\n",
            "17 6.786916360907011e-05\n",
            "18 5.980097791034629e-05\n",
            "19 5.3808073570198325e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VOwJ-pabiLA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "58c54192-97a6-4139-a855-d3ecf4f55aea"
      },
      "source": [
        "pred=n.predict(X_test)\n",
        "np.bincount(n.predict(X_test)),np.bincount(y_test)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([304, 320, 286, 325, 299, 259, 300, 340, 267, 300]),\n",
              " array([296, 327, 305, 326, 305, 283, 282, 336, 252, 288]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxhdkCufcrZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e644aaf-8909-4b84-dacd-4fd3173a95b3"
      },
      "source": [
        "print(f\"accuracy is {np.bincount(np.abs(y_test-pred))[0]*100/len(y_test)} %\")"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy is 91.3 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}